{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import numbers\n",
    "from warnings import catch_warnings, simplefilter, warn\n",
    "import threading\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MAX_INT = np.iinfo(np.int32).max\n",
    "from sklearn.tree._tree import DTYPE, DOUBLE\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "from scipy.sparse import hstack as sparse_hstack\n",
    "from joblib import Parallel\n",
    "from sklearn.ensemble._forest import ForestClassifier, ClassifierMixin, BaseForest, ABCMeta, \\\n",
    "    _get_n_samples_bootstrap, _parallel_build_trees, _generate_sample_indices, DecisionTreeClassifier, _partition_estimators, \\\n",
    "    _accumulate_prediction\n",
    "from sklearn.utils.fixes import delayed\n",
    "# from sklearn.base import _partition_estimators\n",
    "from sklearn.utils.multiclass import check_classification_targets, type_of_target\n",
    "from sklearn.utils import check_random_state, compute_sample_weight, deprecated\n",
    "from sklearn.utils.validation import check_is_fitted, _check_sample_weight, _check_feature_names_in\n",
    "\n",
    "\n",
    "def _accumulate_prediction(predict, X, out, lock):\n",
    "    \"\"\"\n",
    "    This is a utility function for joblib's Parallel.\n",
    "    It can't go locally in ForestClassifier or ForestRegressor, because joblib\n",
    "    complains that it cannot pickle it when placed there.\n",
    "    \"\"\"\n",
    "    prediction = predict(X)\n",
    "    with lock:\n",
    "        if len(out) == 1:\n",
    "            out[0] += prediction\n",
    "        else:\n",
    "            for i in range(len(out)):\n",
    "                out[i] += prediction[i]\n",
    "\n",
    "def _parallel_build_trees(\n",
    "    tree,\n",
    "    bootstrap,\n",
    "    X,\n",
    "    y,\n",
    "    sample_weight,\n",
    "    tree_idx,\n",
    "    n_trees,\n",
    "    verbose=0,\n",
    "    class_weight=None,\n",
    "    n_samples_bootstrap=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Private function used to fit a single tree in parallel.\"\"\"\n",
    "    if verbose > 1:\n",
    "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
    "\n",
    "    if bootstrap:\n",
    "        n_samples = X.shape[0]\n",
    "        if sample_weight is None:\n",
    "            curr_sample_weight = np.ones((n_samples,), dtype=np.float64)\n",
    "        else:\n",
    "            curr_sample_weight = sample_weight.copy()\n",
    "\n",
    "        indices = _generate_sample_indices(\n",
    "            tree.random_state, n_samples, n_samples_bootstrap\n",
    "        )\n",
    "        sample_counts = np.bincount(indices, minlength=n_samples)\n",
    "        curr_sample_weight *= sample_counts\n",
    "\n",
    "        if class_weight == \"subsample\":\n",
    "            with catch_warnings():\n",
    "                simplefilter(\"ignore\", DeprecationWarning)\n",
    "                curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n",
    "        elif class_weight == \"balanced_subsample\":\n",
    "            curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n",
    "\n",
    "        tree.fit(X, y, sample_weight=curr_sample_weight)\n",
    "    else:\n",
    "        tree.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "class MyBaseForest(BaseForest):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if issparse(y):\n",
    "            raise ValueError(\"sparse multilabel-indicator for y is not supported.\")\n",
    "        X, y = self._validate_data(\n",
    "            X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE\n",
    "        )\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = _check_sample_weight(sample_weight, X)\n",
    "\n",
    "        if issparse(X):\n",
    "            X.sort_indices()\n",
    "\n",
    "        y = np.atleast_1d(y)\n",
    "        if y.ndim == 2 and y.shape[1] == 1:\n",
    "            warn(\n",
    "                \"A column-vector y was passed when a 1d array was\"\n",
    "                \" expected. Please change the shape of y to \"\n",
    "                \"(n_samples,), for example using ravel().\",\n",
    "                DataConversionWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "\n",
    "        if y.ndim == 1:\n",
    "\n",
    "            y = np.reshape(y, (-1, 1))\n",
    "\n",
    "        if self.criterion == \"poisson\":\n",
    "            if np.any(y < 0):\n",
    "                raise ValueError(\n",
    "                    \"Some value(s) of y are negative which is \"\n",
    "                    \"not allowed for Poisson regression.\"\n",
    "                )\n",
    "            if np.sum(y) <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Sum of y is not strictly positive which \"\n",
    "                    \"is necessary for Poisson regression.\"\n",
    "                )\n",
    "\n",
    "        self.n_outputs_ = y.shape[1]\n",
    "\n",
    "        y, expanded_class_weight = self._validate_y_class_weight(y)\n",
    "\n",
    "        if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n",
    "            y = np.ascontiguousarray(y, dtype=DOUBLE)\n",
    "\n",
    "        if expanded_class_weight is not None:\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * expanded_class_weight\n",
    "            else:\n",
    "                sample_weight = expanded_class_weight\n",
    "\n",
    "        if not self.bootstrap and self.max_samples is not None:\n",
    "            raise ValueError(\n",
    "                \"`max_sample` cannot be set if `bootstrap=False`. \"\n",
    "                \"Either switch to `bootstrap=True` or set \"\n",
    "                \"`max_sample=None`.\"\n",
    "            )\n",
    "        elif self.bootstrap:\n",
    "            n_samples_bootstrap = _get_n_samples_bootstrap(\n",
    "                n_samples=X.shape[0], max_samples=self.max_samples\n",
    "            )\n",
    "        else:\n",
    "            n_samples_bootstrap = None\n",
    "\n",
    "        self._validate_estimator()\n",
    "        if not self.bootstrap and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        if not self.warm_start or not hasattr(self, \"estimators_\"):\n",
    "            # Free allocated memory, if any\n",
    "            self.estimators_ = []\n",
    "\n",
    "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
    "\n",
    "        if n_more_estimators < 0:\n",
    "            raise ValueError(\n",
    "                \"n_estimators=%d must be larger or equal to \"\n",
    "                \"len(estimators_)=%d when warm_start==True\"\n",
    "                % (self.n_estimators, len(self.estimators_))\n",
    "            )\n",
    "\n",
    "        elif n_more_estimators == 0:\n",
    "            warn(\n",
    "                \"Warm-start fitting without increasing n_estimators does not \"\n",
    "                \"fit new trees.\"\n",
    "            )\n",
    "        else:\n",
    "            if self.warm_start and len(self.estimators_) > 0:\n",
    "                random_state.randint(MAX_INT, size=len(self.estimators_))\n",
    "\n",
    "            trees = [\n",
    "                self._make_estimator(append=False, random_state=random_state)\n",
    "                for i in range(n_more_estimators)\n",
    "            ]\n",
    "\n",
    "            trees = Parallel(\n",
    "                n_jobs=self.n_jobs,\n",
    "                verbose=self.verbose,\n",
    "                prefer=\"threads\",\n",
    "            )(\n",
    "                delayed(_parallel_build_trees)(\n",
    "                    t,\n",
    "                    self.bootstrap,\n",
    "                    X,\n",
    "                    y,\n",
    "                    sample_weight,\n",
    "                    i,\n",
    "                    len(trees),\n",
    "                    verbose=self.verbose,\n",
    "                    class_weight=self.class_weight,\n",
    "                    n_samples_bootstrap=n_samples_bootstrap,\n",
    "                )\n",
    "                for i, t in enumerate(trees)\n",
    "            )\n",
    "\n",
    "            # Collect newly grown trees\n",
    "            self.estimators_.extend(trees)\n",
    "\n",
    "        if self.oob_score:\n",
    "            y_type = type_of_target(y)\n",
    "            if y_type in (\"multiclass-multioutput\", \"unknown\"):\n",
    "                raise ValueError(\n",
    "                    \"The type of target cannot be used to compute OOB \"\n",
    "                    f\"estimates. Got {y_type} while only the following are \"\n",
    "                    \"supported: continuous, continuous-multioutput, binary, \"\n",
    "                    \"multiclass, multilabel-indicator.\"\n",
    "                )\n",
    "            self._set_oob_score_and_attributes(X, y)\n",
    "\n",
    "        if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n",
    "            self.n_classes_ = self.n_classes_[0]\n",
    "            self.classes_ = self.classes_[0]\n",
    "\n",
    "        return self\n",
    "\n",
    "class MyForestClassifier(ClassifierMixin, MyBaseForest, metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator,\n",
    "        n_estimators=100,\n",
    "        *,\n",
    "        estimator_params=tuple(),\n",
    "        bootstrap=False,\n",
    "        oob_score=False,\n",
    "        n_jobs=None,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        class_weight=None,\n",
    "        max_samples=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=estimator_params,\n",
    "            bootstrap=bootstrap,\n",
    "            oob_score=oob_score,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            class_weight=class_weight,\n",
    "            max_samples=max_samples,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_oob_predictions(tree, X):\n",
    "        \"\"\"Compute the OOB predictions for an individual tree.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : DecisionTreeClassifier object\n",
    "            A single decision tree classifier.\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            The OOB samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples, n_classes, n_outputs)\n",
    "            The OOB associated predictions.\n",
    "        \"\"\"\n",
    "        y_pred = tree.predict_proba(X, check_input=False)\n",
    "        y_pred = np.array(y_pred, copy=False)\n",
    "        if y_pred.ndim == 2:\n",
    "            y_pred = y_pred[..., np.newaxis]\n",
    "        else:\n",
    "            y_pred = np.rollaxis(y_pred, axis=0, start=3)\n",
    "        return y_pred\n",
    "\n",
    "    def _set_oob_score_and_attributes(self, X, y):\n",
    "        \"\"\"Compute and set the OOB score and attributes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The data matrix.\n",
    "        y : ndarray of shape (n_samples, n_outputs)\n",
    "            The target matrix.\n",
    "        \"\"\"\n",
    "        self.oob_decision_function_ = super()._compute_oob_predictions(X, y)\n",
    "        if self.oob_decision_function_.shape[-1] == 1:\n",
    "            # drop the n_outputs axis if there is a single output\n",
    "            self.oob_decision_function_ = self.oob_decision_function_.squeeze(axis=-1)\n",
    "        self.oob_score_ = accuracy_score(\n",
    "            y, np.argmax(self.oob_decision_function_, axis=1)\n",
    "        )\n",
    "\n",
    "    def _validate_y_class_weight(self, y):\n",
    "        check_classification_targets(y)\n",
    "\n",
    "        y = np.copy(y)\n",
    "        expanded_class_weight = None\n",
    "\n",
    "        if self.class_weight is not None:\n",
    "            y_original = np.copy(y)\n",
    "\n",
    "        self.classes_ = []\n",
    "        self.n_classes_ = []\n",
    "\n",
    "        y_store_unique_indices = np.zeros(y.shape, dtype=int)\n",
    "        for k in range(self.n_outputs_):\n",
    "            classes_k, y_store_unique_indices[:, k] = np.unique(\n",
    "                y[:, k], return_inverse=True\n",
    "            )\n",
    "            self.classes_.append(classes_k)\n",
    "            self.n_classes_.append(classes_k.shape[0])\n",
    "        y = y_store_unique_indices\n",
    "\n",
    "        if self.class_weight is not None:\n",
    "            valid_presets = (\"balanced\", \"balanced_subsample\")\n",
    "            if isinstance(self.class_weight, str):\n",
    "                if self.class_weight not in valid_presets:\n",
    "                    raise ValueError(\n",
    "                        \"Valid presets for class_weight include \"\n",
    "                        '\"balanced\" and \"balanced_subsample\".'\n",
    "                        'Given \"%s\".'\n",
    "                        % self.class_weight\n",
    "                    )\n",
    "                if self.warm_start:\n",
    "                    warn(\n",
    "                        'class_weight presets \"balanced\" or '\n",
    "                        '\"balanced_subsample\" are '\n",
    "                        \"not recommended for warm_start if the fitted data \"\n",
    "                        \"differs from the full dataset. In order to use \"\n",
    "                        '\"balanced\" weights, use compute_class_weight '\n",
    "                        '(\"balanced\", classes, y). In place of y you can use '\n",
    "                        \"a large enough sample of the full training set \"\n",
    "                        \"target to properly estimate the class frequency \"\n",
    "                        \"distributions. Pass the resulting weights as the \"\n",
    "                        \"class_weight parameter.\"\n",
    "                    )\n",
    "\n",
    "            if self.class_weight != \"balanced_subsample\" or not self.bootstrap:\n",
    "                if self.class_weight == \"balanced_subsample\":\n",
    "                    class_weight = \"balanced\"\n",
    "                else:\n",
    "                    class_weight = self.class_weight\n",
    "                expanded_class_weight = compute_sample_weight(class_weight, y_original)\n",
    "\n",
    "        return y, expanded_class_weight\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class for X.\n",
    "        The predicted class of an input sample is a vote by the trees in\n",
    "        the forest, weighted by their probability estimates. That is,\n",
    "        the predicted class is the one with highest mean probability\n",
    "        estimate across the trees.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, its dtype will be converted to\n",
    "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "\n",
    "        if self.n_outputs_ == 1:\n",
    "            return self.classes_.take(np.argmax(proba, axis=1), axis=0)\n",
    "\n",
    "        else:\n",
    "            n_samples = proba[0].shape[0]\n",
    "            class_type = self.classes_[0].dtype\n",
    "            predictions = np.empty((n_samples, self.n_outputs_), dtype=class_type)\n",
    "\n",
    "            for k in range(self.n_outputs_):\n",
    "                predictions[:, k] = self.classes_[k].take(\n",
    "                    np.argmax(proba[k], axis=1), axis=0\n",
    "                )\n",
    "\n",
    "            return predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for X.\n",
    "        The predicted class probabilities of an input sample are computed as\n",
    "        the mean predicted class probabilities of the trees in the forest.\n",
    "        The class probability of a single tree is the fraction of samples of\n",
    "        the same class in a leaf.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, its dtype will be converted to\n",
    "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
    "            The class probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = self._validate_X_predict(X)\n",
    "\n",
    "        n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n",
    "\n",
    "        all_proba = [\n",
    "            np.zeros((X.shape[0], j), dtype=np.float64)\n",
    "            for j in np.atleast_1d(self.n_classes_)\n",
    "        ]\n",
    "        lock = threading.Lock()\n",
    "        Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
    "            delayed(_accumulate_prediction)(e.predict_proba, X, all_proba, lock)\n",
    "            for e in self.estimators_\n",
    "        )\n",
    "\n",
    "        for proba in all_proba:\n",
    "            proba /= len(self.estimators_)\n",
    "\n",
    "        if len(all_proba) == 1:\n",
    "            return all_proba[0]\n",
    "        else:\n",
    "            return all_proba\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class log-probabilities for X.\n",
    "        The predicted class log-probabilities of an input sample is computed as\n",
    "        the log of the mean predicted class probabilities of the trees in the\n",
    "        forest.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, its dtype will be converted to\n",
    "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
    "            The class probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute :term:`classes_`.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "\n",
    "        if self.n_outputs_ == 1:\n",
    "            return np.log(proba)\n",
    "\n",
    "        else:\n",
    "            for k in range(self.n_outputs_):\n",
    "                proba[k] = np.log(proba[k])\n",
    "\n",
    "            return proba\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"multilabel\": True}\n",
    "    \n",
    "    \n",
    "class CO2_forest(MyForestClassifier):        \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        *,\n",
    "        criterion=\"gini\",\n",
    "        min_samples_split=0,\n",
    "        C=1.0,\n",
    "        kernel=\"linear\",\n",
    "        max_iter:float=1e5,\n",
    "        random_state=None,\n",
    "        tol=1e-4,\n",
    "        degree=3,\n",
    "        gamma=\"scale\",\n",
    "        split_criteria=\"impurity\",\n",
    "        max_features=None,\n",
    "        splitter=\"random\",\n",
    "        normalize=False,\n",
    "        multiclass_strategy=\"ovo\",\n",
    "        bootstrap=True,\n",
    "        oob_score=False,\n",
    "        n_jobs=None,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        class_weight=None,\n",
    "        max_samples=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            base_estimator=Stree(),\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=(\n",
    "                \"C\",\n",
    "                \"kernel\",\n",
    "                \"max_iter\",\n",
    "                \"random_state\",\n",
    "                \"tol\",\n",
    "                \"degree\",\n",
    "                \"gamma\",\n",
    "                \"split_criteria\",\n",
    "                \"criterion\",\n",
    "                \"min_samples_split\",\n",
    "                \"max_features\",\n",
    "                \"splitter\",\n",
    "                \"normalize\",\n",
    "                \"multiclass_strategy\",\n",
    "            ),\n",
    "            bootstrap=bootstrap,\n",
    "            oob_score=oob_score,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            class_weight=class_weight,\n",
    "            max_samples=max_samples,\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.degree = degree\n",
    "        self.gamma = gamma\n",
    "        self.split_criteria = split_criteria\n",
    "        self.criterion = criterion\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.splitter = splitter\n",
    "        self.normalize = normalize\n",
    "        self.multiclass_strategy = multiclass_strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CO2_forest()"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co2 = CO2_forest()\n",
    "co2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, co2.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
